{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text is highly unstructured and needs to be prepared into a form that can be processed by machine learning algorithms. There are several different [approaches](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction) for extracting features from text and we will explore a few of them in the next notebook. However, before extracting features from text, we need to \"preprocess\" it, i.e., \"clean\" and \"standardize\" it. This is because raw text can be \"messy\", especially when coming from social media platforms! We need to keep as many \"informative\" words as possible while discarding the \"uninformative\" ones. Removing unnecessary terms, i.e., the \"noise\", will improve our models' performance.\n",
    "\n",
    "We will be using the [Sentiment140](http://help.sentiment140.com/for-students/) public twitter corpus. This dataset contains ~500 tweets, labeled as positive, negative, or neutral. The dataset is available in the *data* folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os, re, random, string\n",
    "from collections import defaultdict\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "from gensim.models import Phrases\n",
    "\n",
    "# Load helper functions\n",
    "import tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size:  (498, 2)\n"
     ]
    }
   ],
   "source": [
    "# Load corpus (twitter dataset)\n",
    "\n",
    "path = os.path.join('data','sentiment_140.csv')\n",
    "df   = pd.read_csv(path, header=None)\n",
    "\n",
    "# Rename columns\n",
    "df.columns = ['label','tweet']\n",
    "\n",
    "# Re-order columns\n",
    "df = df[['tweet','label']].copy()\n",
    "\n",
    "# Drop duplicates\n",
    "df = df.drop_duplicates('tweet')\n",
    "\n",
    "# Remove empty rows\n",
    "df = df.dropna()\n",
    "\n",
    "# Rename labels\n",
    "label_dict = {0:'neg', 2:'neutral', 4:'pos'}\n",
    "df['label'] = df['label'].replace(label_dict)\n",
    "\n",
    "# Reset index\n",
    "df.reset_index(inplace=True,drop=True)\n",
    "print('Data size: ',df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Take a few minutes to look at the raw text. What do you think we should remove from the text?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>I'm truly braindead.  I couldn't come up with Warren Buffet's name to save my soul</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>Back from seeing 'Star Trek' and 'Night at the Museum.' 'Star Trek' was amazing, but 'Night at the Museum' was; eh.</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>@freitasm oh I see. I thought AT&amp;amp;T were 900MHz WCDMA?</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>cant sleep... my tooth is aching.</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>@SoChi2 I current use the Nikon D90 and love it, but not as much as the Canon 40D/50D. I chose the D90 for the  video feature. My mistake.</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                          tweet  \\\n",
       "450                                                          I'm truly braindead.  I couldn't come up with Warren Buffet's name to save my soul   \n",
       "367                         Back from seeing 'Star Trek' and 'Night at the Museum.' 'Star Trek' was amazing, but 'Night at the Museum' was; eh.   \n",
       "459                                                                                   @freitasm oh I see. I thought AT&amp;T were 900MHz WCDMA?   \n",
       "42                                                                                                            cant sleep... my tooth is aching.   \n",
       "29   @SoChi2 I current use the Nikon D90 and love it, but not as much as the Canon 40D/50D. I chose the D90 for the  video feature. My mistake.   \n",
       "\n",
       "       label  \n",
       "450  neutral  \n",
       "367      neg  \n",
       "459  neutral  \n",
       "42       neg  \n",
       "29       pos  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many different ways for text preprocessing, depending on the application and the text's language. Below are a few suggestions of what we could address in this particular dataset. However, keep in mind that you may have to adapt the techniques discussed in this notebook to your specific case.\n",
    "\n",
    "* remove URLs (e.g., http://bit.ly/19epAH, www.tinyurl.com/m595fk)\n",
    "* remove RT (stands for retweet)\n",
    "* remove Twitter usernames (e.g., @BlondeBroad)\n",
    "* remove hashtags (e.g. #Adidas -> Adidas)\n",
    "* remove punctuation. However, a few groupings, such as `:-), <3, : d`, etc., express emotion, so, depending on the task, we may want to keep them in the text.\n",
    "* remove numbers (e.g. 2020, 2, 15, ...)\n",
    "* perform case conversion (e.g. Good -> good, ...)\n",
    "* remove stopwords\n",
    "* remove non-ASCII characters\n",
    "* standardize the number of repeated characters, e.g. (\"I loooooooovvvvvveee\" -> \"I loovvee\")\n",
    "* expand contractions (e.g. \"don't\" -> \"do not\", \"won't\" -> \"will not\", ...)\n",
    "* apply stemming\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Tokenization is the process of splitting text into **words**.\n",
    "\n",
    "Example:\n",
    "\n",
    "$$\\text{I came to Bern by train.} → \\text{[I, came, to, Bern, by, train.]}$$\n",
    "  \n",
    "  \n",
    "**The *TweetTokenizer***\n",
    "\n",
    "The *TweetTokenizer* is a nice tool from the NLTK library, specially designed for tokenizing tweets. Apart from spliting text into words, it offers a few additional key options:\n",
    "- reduces the number of repeated characters within a token e.g. \"everrrrr\" -> \"everrr\" (use: *reduce_len=True*)\n",
    "- removes Twitter usernames (use: *strip_handles=True*)\n",
    "- preserves punctuation and emoticons.\n",
    "\n",
    "Let's see this with a few examples ([source](https://www.nltk.org/_modules/nltk/tokenize/casual.html#TweetTokenizer))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# Create an instance of the tokenizer\n",
    "tokenizer = TweetTokenizer(reduce_len=True, strip_handles=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <-- \n",
      "\n",
      "['This', 'is', 'a', 'coool', '#dummysmiley', ':', ':-)', ':-P', '<3', 'and', 'some', 'arrows', '<', '>', '->', '<--']\n"
     ]
    }
   ],
   "source": [
    "# Example using emoticons and punctuation\n",
    "\n",
    "sample_1 = \"This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--\"\n",
    "\n",
    "print(sample_1,'\\n')\n",
    "print(tokenizer.tokenize(sample_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@remy: This is waaaaayyyy too much for you!!!!!! \n",
      "\n",
      "[':', 'This', 'is', 'waaayyy', 'too', 'much', 'for', 'you', '!', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "# Example using strip_handles and reduce_len parameters\n",
    "\n",
    "sample_2 = '@remy: This is waaaaayyyy too much for you!!!!!!'\n",
    "\n",
    "print(sample_2,'\\n')\n",
    "print(tokenizer.tokenize(sample_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case folding\n",
    "\n",
    "We usually convert all documents to lowercase. This is because we want our models to count e.g. \"I\" together with \"i\", \"The\" together with \"the\", etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tweet: \t\t\tI don't like this movie!\n",
      "\n",
      "Convert to lowercase: \t\t\ti don't like this movie!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_3 = \"I don't like this movie!\"\n",
    "\n",
    "print('Original tweet: \\t\\t\\t{}\\n'.format(sample_3))\n",
    "\n",
    "# Case folding\n",
    "print('Convert to lowercase: \\t\\t\\t{}\\n'.format(sample_3.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "\n",
    "Stopwords are words that are essential for a sentence to make sense, such as: \"I\", \"the\", \"and\", etc. The issue with stopwords is that they are: *very frequent* and *uninformative*. For most NLP applications, it is a good idea to remove them from text. \n",
    "\n",
    "Most NLP libraries provide pre-compiled lists of stopwords for several languages. In this notebook, we will use the list provided by the [NLTK library](https://www.nltk.org/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/xydi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Load english stopwords from nltk\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords          \n",
    "stopwords_english = stopwords.words('english') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179 stopwords in NLTK's list.\n",
      "\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# Print stopwords\n",
    "\n",
    "print('{} stopwords in NLTK\\'s list.\\n'.format(len(stopwords_english)))\n",
    "print(stopwords_english)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the NLTK stopword list includes negation words such as:\n",
    "\n",
    "- no, nor, not\n",
    "- don't, didn't, wouldn't\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if 'not' is in stopwords_english\n",
    "\n",
    "'not' in stopwords_english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if \"don't\" is in stopwords_english\n",
    "\n",
    "\"don't\" in stopwords_english"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may be an important point to keep in mind, depending on the application. For example, if we use the NLTK stopword list \"out-of-the-box\", then a sentence like:\n",
    "\n",
    "$$\\text{\"I don't like this movie\"}$$\n",
    "\n",
    "will become:\n",
    "$$\\text{\"like movie\"}$$ \n",
    "\n",
    "We see that the processed sentence conveys the exact opposite sentiment from the original one! \n",
    "\n",
    "**We need to be aware of the limitations of pre-compiled stopwords lists; making sure that we adapt them to our particular needs may be necessary for some specific tasks (e.g. sentiment analysis, product reviews, etc.).**\n",
    "\n",
    "We will \"customize\" the NLTK stopword list to ensure that we don't remove negation words from tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove negation words from nltk's stopword list\n",
    "\n",
    "not_stopwords = {'no', 'nor', 'not'} \n",
    "custom_stopwords = set([word for word in stopwords_english if word not in not_stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if 'not' is in custom_stopwords\n",
    "\n",
    "'no' in custom_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, customizing the NLTK stopword lists will not be enough for preserving negation words in the text. We should also expand contractions: e.g. \"don't\" -> \"do not\". We can do this with the library \"expand_contractions\".\n",
    "\n",
    "$$\\text{\"I don't like this movie\"}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tweet: \t\t\tI don't like this movie!\n",
      "\n",
      "Convert to lowercase: \t\t\ti don't like this movie!\n",
      "\n",
      "Expand contractions: \t\t\ti do not like this movie!\n",
      "\n",
      "Tokenize: \t\t\t\t['i', 'do', 'not', 'like', 'this', 'movie!']\n",
      "\n",
      "Remove stopwords: \t\t\t['not', 'like', 'movie!']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import contractions\n",
    "\n",
    "sample_3 = \"I don't like this movie!\"\n",
    "print('Original tweet: \\t\\t\\t{}\\n'.format(sample_3))\n",
    "\n",
    "# Case folding\n",
    "print('Convert to lowercase: \\t\\t\\t{}\\n'.format(sample_3.lower()))\n",
    "\n",
    "# Expand contractions\n",
    "sample_3_expanded = contractions.fix(sample_3.lower())\n",
    "print('Expand contractions: \\t\\t\\t{}\\n'.format(sample_3_expanded))\n",
    "\n",
    "# Tokenize tweet\n",
    "print('Tokenize: \\t\\t\\t\\t{}\\n'.format(sample_3_expanded.split()))\n",
    "\n",
    "# Remove stopwords\n",
    "print('Remove stopwords: \\t\\t\\t{}\\n'.format([w for w in sample_3_expanded.split() if w not in custom_stopwords]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We made sure that the \"important\" words for guessing the sentiment of this tweet (i.e. \"not\" and \"like\") were preserved!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Another part of text normalization is stemming, in which we mainly strip suffixes from the end of the word. The Porter stemmer is a widely used stemming tool for the English language. Stemming helps to connect words and reduce the size of the vocabulary (i.e., the number of unique words in a corpus). However, it can produce non-words, i.e., words that you won't find in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stemmer\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'retriev'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stem of \"retrieve\"\n",
    "\n",
    "stemmer.stem('retrieve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'retriev'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stem of \"retrieval\"\n",
    "\n",
    "stemmer.stem('retrieval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'retriev'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stem of \"retrieved\"\n",
    "\n",
    "stemmer.stem('retrieved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The process_tweet function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's bring everything together and create the `process_tweet` funchion which takes a tweet as an argument and  preprocesses it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create process_tweet function\n",
    "\n",
    "def process_tweet(tweet):\n",
    "    \n",
    "    '''\n",
    "    Preprocess raw samples of tweets.\n",
    "    \n",
    "    INPUT: \n",
    "    - tweet: raw text (string)\n",
    "    \n",
    "    OUTPUT:\n",
    "    - processed_tweet: processed tweet (string)\n",
    "    '''\n",
    "    \n",
    "    # Remove RT\n",
    "    clean_tweet = re.sub(r'RT','',tweet)\n",
    "\n",
    "    # Remove URL\n",
    "    clean_tweet = re.sub(r'https?:\\/\\/[^\\s]+','',clean_tweet)\n",
    "\n",
    "    # Remove hash #\n",
    "    clean_tweet = re.sub(r'#','',clean_tweet)\n",
    "\n",
    "    # Remove numbers\n",
    "    clean_tweet = re.sub(r'\\d+','',clean_tweet)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    clean_tweet = clean_tweet.lower()\n",
    "    \n",
    "    # Remove punctuation repetions (that are not removed by TweetTokenizer)\n",
    "    clean_tweet = re.sub(r'([._]){2,}','',clean_tweet)\n",
    "    \n",
    "    # Remove non-ascii chars\n",
    "    clean_tweet = ''.join([c for c in str(clean_tweet) if ord(c) < 128])\n",
    "\n",
    "    # Expand contractions\n",
    "    clean_tweet = contractions.fix(clean_tweet)\n",
    "    \n",
    "    # Tokenize tweet\n",
    "    tokens = tokenizer.tokenize(clean_tweet)\n",
    "\n",
    "    # Remove punctuation (except emoticons), stopwords, single-char words and apply stemming\n",
    "    clean_tokens = [stemmer.stem(w) for w in tokens if (w not in string.punctuation) and\n",
    "                       (w not in custom_stopwords) and (len(w)>1)]\n",
    "    \n",
    "    # The stemmer strips the final 's but leaves the apostroph: warner's -> warner'\n",
    "    # Here, I'm removing the apostroph from the end of words\n",
    "    clean_tokens = [tok if tok[-1] != \"'\" else tok[:-1] for tok in clean_tokens]\n",
    "\n",
    "    # Join tokens in a single string to recreate the tweet\n",
    "    processed_tweet = ' '.join([tok for tok in clean_tokens])\n",
    "    processed_tweet = processed_tweet.strip()\n",
    "       \n",
    "    return processed_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this function with a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning: \t#jobs #sittercity Help with taking care of sick child (East Palo Alto, CA) http://tinyurl.com/qwrr2m\n",
      "\n",
      "After cleaning: \tjob sitterc help take care sick child east palo alto ca\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample a tweet randomly from the corpus\n",
    "\n",
    "tweet =  df.iloc[random.randint(0,len(df)-1),0]\n",
    "\n",
    "print('Before cleaning: \\t{}\\n'.format(tweet))\n",
    "print('After cleaning: \\t{}\\n'.format(process_tweet(tweet)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now go ahead and process all tweets in the corpus using the `process_tweet` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all tweets (and add to list)\n",
    "\n",
    "processed_tweets = [process_tweet(tweet).split() for tweet in df['tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['looovvvee', 'kindl', 'not', 'dx', 'cool', 'fantast', 'right'],\n",
       " ['read', 'kindl', 'love', 'lee', 'child', 'good', 'read'],\n",
       " ['ok', 'first', 'asses', 'kindl', 'fuck', 'rock']]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show a few samples of processed tweets\n",
    "\n",
    "processed_tweets[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collocations\n",
    "\n",
    "There is one last thing we can do before moving on to feature extraction.\n",
    "\n",
    "We can look for combinations of words that frequently appear together, such as: \"Night at the museum\", \"Star Trek\", \"last night\", \"San Francisco\", \"North Korea\", etc, and replace them by a unique token. \n",
    "\n",
    "Example: \n",
    "\n",
    "$$\\text{\"Star Trek\" → \"Star_Trek\"}$$\n",
    "$$\\text{\"San Francisco\" → \"San_Francisco\"}$$\n",
    "$$\\text{\"North Korea\" → \"North_Korea\"}$$\n",
    "\n",
    "\n",
    "We often call these *phrases* or *collocations*; they are word combinations that are more common in the corpus than the individual words themselves. (*Note: \"that is\" is not considered a collocation*).\n",
    "\n",
    "We will use Gensim's `models.phrases` to detect phrases (collocations) in our corpus. \"Phrases\" will identify the most common collocations and join the constituent tokens into a single token, using the \"_\" glue character. \n",
    "\n",
    "*Documentation*\n",
    "* Gensim's website: https://radimrehurek.com/gensim/models/phrases.html\n",
    "* Mikolov, *et. al*: [\"Distributed Representations of Words and Phrases and their Compositionality\"](https://arxiv.org/pdf/1310.4546.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing collocations (bigrams)...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Detect collocations in corpus\n",
    "\n",
    "for i in ['bigrams']:\n",
    "    print('Computing collocations ({})...'.format(i))\n",
    "    \n",
    "    bigram = gensim.models.Phrases(processed_tweets,   # Expected format: list of tokenized documents\n",
    "                                   min_count=3,        # Ignore all words and bigrams with total collected count lower than this value.\n",
    "                                   delimiter=b'_')     # Glue character used to join collocation tokens\n",
    "\n",
    "    bigram_model = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "    # Add detected collocations to corpus\n",
    "    processed_tweets = [' '.join(bigram_model[processed_tweet]) for processed_tweet in processed_tweets]\n",
    "    print('Done!')\n",
    "    \n",
    "# Add processed tweets to dataframe\n",
    "df['processed_tweet'] = processed_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size:  (498, 3)\n"
     ]
    }
   ],
   "source": [
    "# Remove empty tweets\n",
    "df = df[df['processed_tweet'].apply(len) != 0].copy()\n",
    "\n",
    "# Reindex dataframe\n",
    "df.reset_index(inplace=True,drop=True)\n",
    "\n",
    "# Re-order columns\n",
    "df = df[['tweet','processed_tweet','label']]\n",
    "\n",
    "print('Data size: ',df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how the processed tweets look like: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>processed_tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>Beginning JavaScript and CSS Development with jQuery #javascript #css #jquery http://bit.ly/TO3e5</td>\n",
       "      <td>begin javascript css develop jqueri javascript css jqueri</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>Wow everyone at the Google I/O conference got free G2's with a month of unlimited service</td>\n",
       "      <td>wow everyon googl confer got free g month unlimit servic</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>@crlane I have the Kindle2. I've seen pictures of the DX, but haven't seen it in person. I love my Kindle - I'm on it everyday.</td>\n",
       "      <td>kindl seen pictur dx not seen person love kindl everyday</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>@anna_debenham what was the php jquery hack?</td>\n",
       "      <td>php jqueri hack</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>We went to Stanford University today. Got a tour. Made me want to go back to college. It's also decided all of our kids will go there.</td>\n",
       "      <td>went stanford univers today got tour made want go back colleg also decid kid go</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                      tweet  \\\n",
       "201                                       Beginning JavaScript and CSS Development with jQuery #javascript #css #jquery http://bit.ly/TO3e5   \n",
       "353                                               Wow everyone at the Google I/O conference got free G2's with a month of unlimited service   \n",
       "265         @crlane I have the Kindle2. I've seen pictures of the DX, but haven't seen it in person. I love my Kindle - I'm on it everyday.   \n",
       "199                                                                                            @anna_debenham what was the php jquery hack?   \n",
       "218  We went to Stanford University today. Got a tour. Made me want to go back to college. It's also decided all of our kids will go there.   \n",
       "\n",
       "                                                                     processed_tweet  \\\n",
       "201                        begin javascript css develop jqueri javascript css jqueri   \n",
       "353                         wow everyon googl confer got free g month unlimit servic   \n",
       "265                         kindl seen pictur dx not seen person love kindl everyday   \n",
       "199                                                                  php jqueri hack   \n",
       "218  went stanford univers today got tour made want go back colleg also decid kid go   \n",
       "\n",
       "       label  \n",
       "201  neutral  \n",
       "353      pos  \n",
       "265      pos  \n",
       "199  neutral  \n",
       "218      pos  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <strong>Task : Find out which collocations were detected in the corpus!</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small exercise : Practice how to count word occurencies\n",
    "\n",
    "# Sort collocations by frequency of appearence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_collocations = []\n",
    "for tweet in df['processed_tweet']:\n",
    "    for word in tweet.split():\n",
    "        if '_' in word:\n",
    "            list_collocations.append(word)\n",
    "            \n",
    "pd.value_counts(list_collocations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collocations_dict = defaultdict(int)\n",
    "\n",
    "for tweet in df['processed_tweet']:\n",
    "    for word in tweet.split():\n",
    "        if '_' in word:\n",
    "            collocations_dict[word] += 1\n",
    "            \n",
    "# Sort counts in descending order\n",
    "collocations_dict = {k:v for k,v in sorted(collocations_dict.items(), \n",
    "                                         key=lambda item:item[1], reverse=True)}\n",
    "            \n",
    "print('Found: {} collocations in corpus\\n'.format(len(list(collocations_dict.keys()))))\n",
    "\n",
    "# Print collocations (and frequency)\n",
    "collocations_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually label tweets into \"semantic\" categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of a later analysis, we will now label tweets manually, based on selected keywords.\n",
    "\n",
    "For example, we will add any tweet mentioning movie-related terms (e.g. \"Night at the museum\", \"Star Trek\", \"movie\", etc.) to the category: \"movies\". Tweets mentioning politics-related terms (e.g. \"north_korea\", \"obama\", \"pelosi\", \"bush\", \"china\", \"india\", \"iran\", etc.) will be added to \"politics\". Tweets mentionning sports-related terms (e.g. \"lebron\", \"laker\", \"basebal\", \"basketbal\", \"fifa\", \"ncaa\", \"roger\", \"feder\", etc.) will be added to \"sports\". And so on.\n",
    "\n",
    "We will use the function `add_to_semantic_category` to label tweets manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function add_to_semantic_category() \n",
    "\n",
    "df = tools.add_to_semantic_category(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a few samples\n",
    "\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data\n",
    "\n",
    "df.to_csv('data/clean_sentiment_140.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
