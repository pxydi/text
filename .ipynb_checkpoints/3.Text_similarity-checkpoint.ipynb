{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title << Setup Google Colab by running this cell {display-mode: \"form\"}\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    # Clone GitHub repository\n",
    "    !git clone https://github.com/pxydi/text.git\n",
    "        \n",
    "    # Copy files required to run the code\n",
    "    !cp -r \"text/data\" \"text/plots\" \"text/tools.py\" .\n",
    "    \n",
    "    # Install packages via pip\n",
    "    !pip install -r \"text/colab-requirements.txt\"\n",
    "    \n",
    "    # Restart Runtime\n",
    "    import os\n",
    "    os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text similarity\n",
    "\n",
    "In the previous notebook, we transformed samples of text into lists of numbers using various methods. In each of these methods, the numbers in the vectors are defined somehow differently: \n",
    "* in Bag of words, they correspond to word counts.\n",
    "* in Tf-idf, they correspond to word counts, re-weighted by the *inverse document frequency*.\n",
    "* in sentence embeddings, they are learned from the data using an \"embedding method\" in such a way that they encode semantic relationships; similar texts have similar embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize data\n",
    "\n",
    "We already mentionned that we can think of these lists of numbers as points in (a high-dimensional) space. We can plot them and try to look for patterns, i.e. clusters of similar documents.\n",
    "\n",
    "However, we can only plot in 2 or 3 dimensions, not more. This means that we need to perform \"dimensionality reduction\", which consists in \"compressing\" the data into few 2 (or 3) dimensions without losing too much information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random, re, os\n",
    "import contractions\n",
    "\n",
    "import tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "path = os.path.join('data','clean_sentiment_140.csv')\n",
    "df   = pd.read_csv(path)\n",
    "\n",
    "# Show a few samples\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load word vectors\n",
    "\n",
    "with np.load('data/word_vectors.npz',allow_pickle=False) as data:\n",
    "    X_bow   = data['BOW_features']\n",
    "    X_tfidf = data['tfidf_features']\n",
    "    X_embed = data['embeddings']\n",
    "    \n",
    "print('Data loaded.')\n",
    "print('BOW features: ',X_bow.shape)\n",
    "print('Tf-idf features: ',X_tfidf.shape)\n",
    "print('Embeddings: ',X_embed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize data\n",
    "\n",
    "Machine learning can help with data visualization. A few popular techniques are\n",
    "\n",
    "- [Principal component analysis](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)\n",
    "- [Truncated SVD](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html)\n",
    "- [T-SNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "decomp = make_pipeline(PCA(n_components=40), TSNE(n_components=2))\n",
    "\n",
    "# BOW features\n",
    "X_decomp_bow   = decomp.fit_transform(X_bow)\n",
    "\n",
    "# Embeddings\n",
    "X_decomp_embed = decomp.fit_transform(X_embed)\n",
    "\n",
    "print(X_decomp_bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "fig,axes = plt.subplots(1,2,figsize=(12,5))\n",
    "\n",
    "#Plot X_decomp_bow\n",
    "axes[0].scatter(X_decomp_bow[:,0],X_decomp_bow[:,1],alpha=0.8)\n",
    "axes[0].set_xlabel('TSNE 1')\n",
    "axes[0].set_ylabel('TSNE 2');\n",
    "axes[0].set_title('Bag of words');\n",
    "\n",
    "#Plot X_decomp_embed\n",
    "axes[1].scatter(X_decomp_embed[:,0],X_decomp_embed[:,1],alpha=0.8)\n",
    "axes[1].set_xlabel('TSNE 1')\n",
    "axes[1].set_ylabel('TSNE 2');\n",
    "axes[1].set_title('Embeddings');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's focus ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_labels = ['movies','Twitter', 'politics', 'sports', 'IT', 'books']\n",
    "\n",
    "tools.visualize_bow_embeddings(X_decomp_bow,X_decomp_embed,df,label='books')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each point in the plot represents a tweet. Using ML here has allowed grouping together tweets that are discussing the same topic. For example, we can see that tweets talking about Obama, North Korea, Iran, or China (i.e. politics) are grouped in the lower right corner of the plot. Similarly, tweets about (American) cable tv (Time Warner, ESPN, or Comcast) all appear together in the top right corner. In fact, there are many such groups in this plot (about movies, sports, food, etc).\n",
    "What I like about it is that an algorithm has managed to capture semantic relationships between tweets, even if the tweets don’t use the same words. The ML algorithms used are capable of detecting similarities between e.g. “Night at the museum” and “Star Trek”, and then putting them close to each other in the plot. We, humans, would have to read all of these tweets (several thousand) and assign them to different groups one by one. ML can do this in two lines of code!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, Bag of words, will determine similarity based on the words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance between vectors\n",
    "\n",
    "Two vectors are similar if they point in the same direction. In a word space, this means that two documents use the same words, in the same proportions, hence they are likely to be discussing the same thing.\n",
    "\n",
    "The idea is that if documents use the same words in the same proportions, then their document vecrors will be closeby in the word space; they will be pointing in the same direction and having similar lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy vectors\n",
    "\n",
    "doc1 = np.array([1,3])\n",
    "doc2 = np.array([16,2])\n",
    "doc3 = np.array([18,5])\n",
    "\n",
    "docs = np.zeros((1,2))\n",
    "\n",
    "for doc in [doc1,doc2,doc3]:\n",
    "    docs = np.vstack((docs,doc))\n",
    "    \n",
    "docs = docs[1:]\n",
    "tools.plot_vectors(doc1,doc2,doc3,plot_difference=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare vectors using: \n",
    "* the euclidean distance\n",
    "* the cosine similarity\n",
    "\n",
    "### Euclidean distance \n",
    "\n",
    "The euclidean distance is simply the distancee between two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_labels = ['doc'+str(i+1) for i in range(0,3)]\n",
    "column_labels = index_labels\n",
    "\n",
    "pd.DataFrame(euclidean_distances(docs), index = index_labels, columns = column_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the euclidean distance can be misleading if used to compare vectors of different lengths. It's better to use the Euclidean distance with vectors of the same length. We will fist normalize the document vectors (ensuring they all have a length of 1) and will recompute the Euclidean distance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot normalized vectors\n",
    "\n",
    "tools.plot_vectors(tools.normalize_vector(doc1),\n",
    "                   tools.normalize_vector(doc2),\n",
    "                   tools.normalize_vector(doc3),\n",
    "                   plot_difference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute euclidean distances using normalized vectors\n",
    "\n",
    "pd.DataFrame(euclidean_distances(tools.normalize_vector(docs)), index = index_labels, columns = column_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity\n",
    "\n",
    "We saw that the Euclidean distance can be affected by the vectors' length. The cosine similarity is another commonly used metric to mesure document similarity, which isn't affected by the vectors' length.\n",
    "\n",
    "The cosine similarity expresess the cosine of the angle between two vectors. \n",
    "\n",
    "* If the angle between two vectors is small (𝜽 -> 0), then the cosine of 𝜽 will be close to 1. \n",
    "* If the angle between two vectors is big (𝜽 -> 90), then the cosine of 𝜽 will be close to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the cosine similarity\n",
    "\n",
    "pd.DataFrame(cosine_similarity(docs), index = index_labels, columns = column_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(euclidean_distances(tools.normalize_vector(docs)), index = index_labels, columns = column_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare our tweets using the Twitter dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features Features\n",
    "\n",
    "X = X_embed\n",
    "X[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try also with cosine similarity\n",
    "#similarity_df = pd.DataFrame(euclidean_distances(normalize_vector(X)))\n",
    "similarity_df = pd.DataFrame(cosine_similarity(X))\n",
    "\n",
    "similarity_df.index   = ['doc_'+str(i) for i in range(0,len(X))]\n",
    "similarity_df.columns = ['doc_'+str(i) for i in range(0,len(X))]\n",
    "\n",
    "similarity_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot similarity metric using heatmaps\n",
    "\n",
    "tools.plot_similarity(similarity_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_neighbohrs(idx,X,data = df):\n",
    "    \n",
    "    neighbohrs_df = pd.DataFrame()\n",
    "    \n",
    "    df_cos  = pd.DataFrame(cosine_similarity(X))\n",
    "    df_dist = pd.DataFrame(euclidean_distances(tools.normalize_vector(X)))\n",
    "    \n",
    "    # Sort neighbors with respect to cosine similarity\n",
    "    neighborhs = np.argsort(df_cos.iloc[idx,:])[::-1]\n",
    "    \n",
    "    neighbohrs_df['processed_tweet'] = df.iloc[neighborhs,1]\n",
    "    neighbohrs_df['cosine_similarity'] = df_cos.iloc[idx,neighborhs]\n",
    "    neighbohrs_df['euclidean_distance'] = df_dist.iloc[idx,neighborhs]\n",
    "    neighbohrs_df['label'] = df.iloc[neighborhs,-2]\n",
    "    neighbohrs_df['semantic_category'] = df.iloc[neighborhs,-1]\n",
    "\n",
    "    return neighbohrs_df.head(10)\n",
    " \n",
    "# Randomly sampled tweet\n",
    "\n",
    "idx = random.randint(0,len(df)-1)\n",
    "print('Doc idx: {}'.format(idx))\n",
    "\n",
    "pd.set_option('max_colwidth', None)\n",
    "find_neighbohrs(idx,X_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(X_tfidf,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(X_bow[np.argsort(np.linalg.norm(X_bow,axis=1))][0:50],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(X_tfidf[np.argsort(np.linalg.norm(X_tfidf,axis=1))][0:50],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argsort(np.linalg.norm(X_bow,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[[113, 117, 346,  66]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[[113,  65,  66, 117, 346]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
