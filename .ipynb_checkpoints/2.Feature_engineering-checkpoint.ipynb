{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title << Setup Google Colab by running this cell {display-mode: \"form\"}\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    # Clone GitHub repository\n",
    "    !git clone https://github.com/pxydi/text.git\n",
    "        \n",
    "    # Copy files required to run the code\n",
    "    !cp -r \"text/data\" \"text/plots\" \"text/tools.py\" .\n",
    "    \n",
    "    # Install packages via pip\n",
    "    !pip install -r \"text/colab-requirements.txt\"\n",
    "    \n",
    "    # Restart Runtime\n",
    "    import os\n",
    "    os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering for text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several different [approaches](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction) for extracting features from text. In this notebook, we will explore a few of them and discuss their strengths and weaknesses. Â \n",
    "\n",
    "We will be using the ***cleaned*** [Sentiment140](http://help.sentiment140.com/for-students/) twitter dataset that we prepared in the previous notebook. You can find it in the *data* folder.\n",
    "\n",
    "We will be looking at the following feature extraction methods for text:\n",
    "\n",
    "* Bag of words and Bag of n-grams  \n",
    "* Tf-idf term weighting  \n",
    "* Sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os, re, random, string\n",
    "from collections import defaultdict\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "from gensim.models import Phrases\n",
    "\n",
    "# Load helper functions\n",
    "import tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed dataset)\n",
    "\n",
    "path = os.path.join('data','clean_sentiment_140.csv')\n",
    "df   = pd.read_csv(path)\n",
    "\n",
    "print('Data size: ',df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a few samples\n",
    "\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computers don't understand natural language. So, how do we represent text?\n",
    "\n",
    "One of the simplest but effective and commonly used models to represent text for machine learning is the ***Bag of Words*** model ([online documentation](https://en.wikipedia.org/wiki/Bag-of-words_model)). When using this model, we discard most of the input text structure (word order, chapters, paragraphs, sentences, and formating) and only count how often each word appears in each text. Discarding the structure and counting only word occurrences leads to the mental image of representing text as a \"bag\". Â \n",
    "\n",
    "Computing the ***Bag of words*** representation for a corpus of documents consists of the following three steps:\n",
    "\n",
    "* **Tokenization**: Split each document into the words that appear in it (called *tokens*), for example, by splitting them on whitespace.\n",
    "* **Vocabulary building**: Collect all unique words that appear in any of the documents.\n",
    "* **Count frequencies**: For each document, count how often each of the vocabulary words appears in this document.\n",
    "\n",
    "The ***Bag of words*** model can be implemented with the *CountVectorizer* object from [*scikit-learn*](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "my_regexp = '(?u)[a-z0-9:(_\\)/;=\\]\\[<>-][a-z0-9:(_\\)/;=\\]\\[<>-]+'\n",
    "\n",
    "# Create an instance of CountVectorizer\n",
    "vectorizer = CountVectorizer(token_pattern=my_regexp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: The CountVectorizer extracts tokens using the default regular expression (`(?u)\\b\\w\\w+\\b`). This regex pattern matches tokens of at least 2 alphanumeric characters, separated by word boundaries. This particular regular expression ignores the punctuation and treats it as a token separator. This doesn't suit our needs, as we would like a tokenizer which preserves emoticons (which are groupings of punctuation marks).*\n",
    "\n",
    "*Therefore, we had to modify the `token_pattern` to keep emoticons in text.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the Bag of words model on a toy corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show toy corpus (before preprocessing)\n",
    "\n",
    "print('Toy corpus:\\n')\n",
    "list(df['tweet'].iloc[[62,410,345]].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy corpus (after cleaning)\n",
    "\n",
    "X_toy = df['processed_tweet'].iloc[[62,410,345]].values\n",
    "\n",
    "# Show toy corpus\n",
    "print('Toy corpus (after cleaning):\\n')\n",
    "print(list(X_toy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_bow: the bag-of-words representation of the tweets\n",
    "\n",
    "X_bow_toy = vectorizer.fit_transform(X_toy)\n",
    "\n",
    "# Show Bag of words vectors\n",
    "df_BOW_toy = pd.DataFrame(X_bow_toy.toarray(), columns=vectorizer.get_feature_names())\n",
    "df_BOW_toy['Text'] = X_toy\n",
    "df_BOW_toy.set_index('Text',inplace=True)\n",
    "\n",
    "print('Shape:      {}'.format(df_BOW_toy.shape))\n",
    "df_BOW_toy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of words has converted all documents into lists of word counts.   \n",
    "* Each column represents a word in the corpus.  \n",
    "* Each row represents a document in the corpus.  \n",
    "* Each cell represents the number of times a particular word (defined by a column) appears in a particular document (defined by a row). \n",
    "\n",
    "For example, \n",
    "\n",
    "* **:/** appears once in the second document and zero times in the other documents.  \n",
    "* **hello twitter api ;)** is represented by this list of numbers: [0,1,0,0,0,1,0,0,0,0,1].\n",
    "\n",
    "\n",
    "We describe each sample of text by word occurencies, completely ignoring the relative position of words in a document. The \"meaning\" of a document is defined as a list of numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question : The shape of `X_bow_toy` is: (3,11). What does each dimension correspond to?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here\n",
    "#\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's time to apply the Bag of words model on the whole corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create array of texts\n",
    "X = df['processed_tweet'].values\n",
    "\n",
    "# Show a few samples from the corpus\n",
    "print('Corpus:\\n')\n",
    "list(X[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of CountVectorizer\n",
    "vectorizer = CountVectorizer(token_pattern=my_regexp)\n",
    "\n",
    "# Apply BoW to corpus\n",
    "X_bow = vectorizer.fit_transform(X)\n",
    "\n",
    "print('Representing text as bag-of-words:\\n')\n",
    "print('Bag of words vectors:      {}'.format(X_bow.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bag of words representation of the train data is a matrix of shape (498, 1643). Each row corresponds to a tweet and each column to a word in the *vocabulary*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question : What is the size of the vocabulary?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here\n",
    "#\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â Solution\n",
    "\n",
    "# Lets have a look at the vocabulary\n",
    "\n",
    "vocab = vectorizer.get_feature_names()\n",
    "print('Vocabulary size: {}\\n'.format(len(vocab)))\n",
    "\n",
    "print('Show some words from the vocabulary:\\n')\n",
    "print(vocab[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Bag of words vectors\n",
    "\n",
    "df_BOW = pd.DataFrame(X_bow.toarray(), columns=vectorizer.get_feature_names())\n",
    "df_BOW.index = ['doc_'+str(i) for i in range(0,len(df_BOW))]\n",
    "df_BOW.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a typical corpus, the **Bag of words** model creates vectors that are ***long*** (10K - 1M) and ***sparse*** (most values are zero). This requires a lot of space and computational time to process. \n",
    "\n",
    "Later we will see an alternative method for representing documents as vectors that are ***short*** (of length 100 - 1'000) and ***dense*** (most values are non-zero)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question : How does min_df, max_df and max_features affect the size of the vocabulary?**\n",
    "\n",
    "We can adjust the size of the vocabulary (hence the size of the vectors) by tuning the following (hyper)parameters: \n",
    "- max_features  \n",
    "- min_df  \n",
    "- max_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(min_df = 2,          # keep words that appear in at least min_df docs\n",
    "                             max_df = 0.9,        # keep words that appear in at less than X% of docs\n",
    "                             max_features = 300,  # keep only top-X most frequent words in the vocabulary\n",
    "                             token_pattern=my_regexp)\n",
    "\n",
    "# Apply BoW to corpus\n",
    "X_bow = vectorizer.fit_transform(X)\n",
    "\n",
    "print('Representing text as bag-of-words:\\n')\n",
    "print('Train set:      {}'.format(X_bow.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Bag of words vectors\n",
    "\n",
    "df_BOW = pd.DataFrame(X_bow.toarray(),columns=vectorizer.get_feature_names())\n",
    "df_BOW.index = ['doc_'+str(i) for i in range(0,len(df_BOW))]\n",
    "df_BOW.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Bag-of-words** \n",
    ">\n",
    ">âœ… Simple   \n",
    ">âœ… Effective and commonly used to represent text for machine learning  \n",
    ">\n",
    ">\n",
    ">ðŸ”´ Discards word order  \n",
    ">ðŸ”´ Creates long (typical vocabulary  sizes: 20K - 1M) and sparse (mostly filled with zeros) vectors  \n",
    ">ðŸ”´ Treats words as distinct \"atomic\" symbols; no natural notion of \"word similarity\" (e.g. \"size\" â‰  \"capacity\" or \"hotel\" â‰  \"motel\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the disadvantages of the bag of words model is that word order is discarded. However, some information about the context of a text may be lying in combinations of words, not just single words. One way to capture context with the bag of words model is by considering pairs or triplets of words appearing next to each other in a sentence. Single words are called ***unigrams***, pairs of words ***bigrams***, triplets of words ***trigrams***, and more generally, sequences of tokens are called ***n-grams***. \n",
    "\n",
    "N-grams retain more of the original sequence structure in the text and can, in some cases, be more \"informative\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example** : Let's use the Bag of n-grams model with the toy corpus from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(X_toy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of n-grams\n",
    "\n",
    "pd.set_option('display.max_columns', 21)\n",
    "vectorizer_n_grams = CountVectorizer(ngram_range=(1, 2),     # Considers unigrams and bigrams\n",
    "                                     token_pattern=my_regexp) \n",
    "\n",
    "# X_ngram_toy: the bag of n-grams representation of the toy corpus\n",
    "X_ngram_toy = vectorizer_n_grams.fit_transform(X_toy)\n",
    "\n",
    "# Show Bag of words vectors\n",
    "df_ngram_toy = pd.DataFrame(X_ngram_toy.toarray(), columns=vectorizer_n_grams.get_feature_names())\n",
    "df_ngram_toy['Text'] = X_toy\n",
    "df_ngram_toy.set_index('Text',inplace=True)\n",
    "print('Shape:      {}'.format(df_ngram_toy.shape))\n",
    "df_ngram_toy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the Bag-of-Words model, we used to have a vocabulary of size 11 (toy corpus). With the Bag of n-grams, the size of the vocabulary is 19."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example** : Now, it's time to apply the bag of n-grams model to the whole corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['processed_tweet'].values\n",
    "\n",
    "# Show a few samples from the corpus\n",
    "print('Corpus:\\n')\n",
    "list(X[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of CountVectorizer\n",
    "vectorizer_n_grams = CountVectorizer(ngram_range=(1, 2),     # Considers unigrams and bigrams\n",
    "                                     token_pattern=my_regexp) \n",
    "\n",
    "# X_ngram_toy: the bag of n-grams representation of the entire corpus\n",
    "X_ngram = vectorizer_n_grams.fit_transform(X)\n",
    "\n",
    "# Show Bag of words vectors\n",
    "df_ngram = pd.DataFrame(X_ngram.toarray(),columns=vectorizer_n_grams.get_feature_names())\n",
    "df_ngram['Text'] = X\n",
    "df_ngram.set_index('Text',inplace=True)\n",
    "print('Size:      {}'.format(df_ngram.shape))\n",
    "df_ngram.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the bag of words model, we used to have a vocabulary of size 1'638. With the bag of n-grams, we have a vocabulary of size 4'743."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** : Mention a few ways for reducing the size of the vocabulary. Can you implement them in the next cell?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of n-grams model with a reduced-size vocabulary\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Bag-of-N-grams**  \n",
    ">\n",
    "> âœ… Retains more information about the context, than just single words.  \n",
    ">\n",
    "> ðŸ”´ Bag-of-n-grams leads to even bigger and sparser vectors (than bag of words).  \n",
    "> ðŸ”´ Long n-grams do not always lead to improvements in model performance. We usually stop at *bigrams* or *trigrams*. Longer n-grams are rarely used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-idf term weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words that are very frequent across *all documents* in a corpus aren't necessarily \"informative\". Imagine, for example, that we are working with a collection of documents about the Coronavirus disease 2019. We naturally expect terms such as *coronavirus*, *COVID-19*, *virus*, etc., to appear very often in all documents in our corpus. However, these terms aren't necessarily conveying any \"information\" about our documents' specific content, which may be about various topics such as vaccines, measures to contain the epidemic, testing, etc. \n",
    "Therefore, we may want to:\n",
    "* Diminish the importance of widespread words over the entire corpus as they could shadow rarer yet more interesting terms.\n",
    "* Focus more on words that are rarer across the corpus, as they could be more informative about the content of documents.\n",
    "\n",
    "This is the purpose of ***tf-idf***: to re-weight word frequencies (tf) by assigning higher idf weights to rare words in the corpus and lower idf weights to terms that are widespread in the corpus. Idf stands for inverse document frequency and is a \"measure\" of a word's \"rarity\" in the corpus. The higher the idf, the \"rarer\" the word in the corpus.\n",
    "\n",
    "* Words that are rare in the corpus (df -> 0) are assigned very high idf weights. \n",
    "* Widespread words in the corpus (df -> 100) are given very low idf weights. \n",
    "\n",
    "**In short, the *tf-idf* algorithm favors terms that are *frequent* in *few documents*.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"plots/idf_plot.png\" style=\"width:500px; height:300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the *tf-idf* model, raw word frequencies are multiplied by the inverse document frequency (idf). The resulting tf-idf vectors are normalized by the lenght of the vector.\n",
    "\n",
    "The *Tf-idf* algorithm can be implemented with the *TfidfVectorizer* object from [*scikit-learn*](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html).\n",
    "\n",
    "**Example** : Let's use it with our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighting text data with Tf-Idf\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df = 2,          # keep words that appear in at least min_df docs\n",
    "                        max_df = 0.9,        # keep words that appear in at less than X% of docs\n",
    "                        max_features = 300,  # keep only top-X most frequent  words in the vocabularytoken_pattern=my_regexp)  \n",
    "                        token_pattern=my_regexp)  \n",
    "\n",
    "# X_train_tf-idf: the tf-idf representation of the train data\n",
    "X_tfidf = tfidf.fit_transform(df['processed_tweet'])\n",
    "\n",
    "# Show Tf-idf words vectors\n",
    "df_tfidf = pd.DataFrame(X_tfidf.toarray(),columns=tfidf.get_feature_names())\n",
    "df_tfidf.index = ['doc_'+str(i) for i in range(0,len(df_tfidf))]\n",
    "df_tfidf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** : Compute the Euclidean norm of the tf-idf vectors."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Hints:\n",
    "# The length of a vector is given by the Euclidean norm.\n",
    "# We can compute the Euclidean norm with the `norm` function from the `numpy.linalg` library.\n",
    "# tf-idf representations are stored as sparse matrices. To look and manipulate these matrices, \n",
    "# we need to convert them to dense matrices with the \"toarray()\" method.\n",
    "# Use: axis=0 for columns and axis=1 for rows\n",
    "\n",
    "np.linalg.norm(X_tfidf.toarray(),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Tf-idf term weighting** \n",
    ">\n",
    "> âœ… Favors terms that are *frequent* in *few* documents.  \n",
    "> âœ… Tf-idf vectors are normalized, unlike bag-of-words vectors. Their values range between 0 and 1.    \n",
    "> âœ… Tf-idf was originally a term weighting scheme developed for information retrieval (as a ranking function for search engines results) that has also found good use in document classification and clustering.\n",
    "> \n",
    "> ðŸ”´ Like the Bag-of-words model, tf-idf results in long vectors that are sparse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings :: A few words\n",
    "\n",
    "So far, we saw how to construct simple representations of text based on word frequencies. While simple to construct, these representations are sparse, very high-dimensional, and have no notion about *word meaning* or *word similarity*. \n",
    "\n",
    "Another very popular way to obtain vector representations of text is through *word embeddings*. Embeddings are *short* (typical sizes: 100-1'000), *dense* (contain mostly non-zero values) and \"embed\" meaning.\n",
    "\n",
    "Word embeddings are *learned* from the data using neural networks by looking at the context around a certain word. This allows to capture the \"relative meaning\" of a word. The \"quality\" of word vectors increases significantly with the amount of training data. Embeddings also are available as \"off-the-shelf\"  \"pre-trained\" embeddings. They are usually trained on large, general purpose corpora (e.g. Wikipedia) or more specialized corpora (\"context\" matters!). \n",
    "\n",
    "\n",
    "Word embeddings can be used to:\n",
    "* calculate similarity between words (e.g., $\\text{plant â‰ˆ flower}$, and $\\text{puppy â‰ˆ dog}$). This is extremely useful as we can find similarities in meaning between text samples even if they don't use the same words.\n",
    "* find analogies between words (e.g., $\\text{Paris is to France as Rome is to ?}$)\n",
    "\n",
    "There are many established methods for computing word embeddings such as: [word2vec](https://code.google.com/archive/p/word2vec/), [GloVe](https://nlp.stanford.edu/projects/glove/), [FastText](https://fasttext.cc/), ... The general idea behind these methods is that two words are more similar if they share similar contexts. If two unique words are frequently surrounded by similar words, in various sentences, then these two words are related in their meaning; they are semantically related. \n",
    "\n",
    "> âœ… Similar words have similar word embeddings.  \n",
    "> âœ… The distance between word embeddings reflects the semantic relationship between these words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"plots/word2vec_PCA_blue.png\" style=\"width:630px; height:330px;\">\n",
    "\n",
    "*A two-dimensional projection of word2vec embeddings for selected words showing that semantically related words are closeby in space.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is another very famous example of word analogies using the word2vec model [(see paper by Mikolov et al.)](https://papers.nips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf):\n",
    "\n",
    "$$kingâˆ’man+woman=queen$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image1](plots/word_analogies_word2vec.png)\n",
    "\n",
    "*A two-dimensional projection of word2vec embeddings for selected words showing that embeddings can capture that the main difference between king and queen is the same as the difference between man and woman.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Word embeddings**\n",
    ">  \n",
    ">âœ…  Word embeddings are short (typical sizes: 100-1'000) and dense (contain mostly non-zero values).  \n",
    ">âœ…  Similar words have similar word embeddings.   \n",
    ">âœ…  The distance between word embeddings reflects the semantic relationship between these words.  \n",
    ">âœ…  Word embeddings are learned from text corpora. Their quality increases with the amount of training data.  \n",
    ">âœ…  Word embeddings are also available as off-the-shelf pre-trained vectors.  \n",
    ">\n",
    ">ðŸ”´ A word of caution: The embeddings we discuss here are trained on \"similarity\" tasks. This means that they are \"optimized\" to minimize the distinctiveness of the vector representations between \"similar\" words. These embeddings are useful for establishing similarities between words, sentences or documents. However, these embeddings are not necessarily \"optimal\" to use with classification tasks as they are not designed to maximize the distinctiveness between vectors from different classes (to achieve maximum accuracy). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Universal Sentence Encoder :: Sentence embeddings\n",
    "\n",
    "Word embeddings can be averaged together to make vector representations of phrases and sentences. Sentence embeddings  are the foundation of more complex NLP applications, such machine translation or question-answering. Moreover, you may have heard of other more advanced word embedding methods ([BERT](https://en.wikipedia.org/wiki/BERT_(language_model)), [GPT-3](https://en.wikipedia.org/wiki/GPT-3), ...): these methods use advanced deep neural network architectures to refine the representation of the word meanings according to their contexts.\n",
    "\n",
    "We will now use the **Universal Sentence Encoder Cross-lingual (XLING) module** which was trained on English, French, German, Spanish, Italian, Chinese, Korean, and Japanese tasks (8 languages) [[paper](https://arxiv.org/pdf/1803.11175.pdf)].\n",
    "\n",
    "The input to the module is variable length text in any of the eight aforementioned languages and the output is a 512 dimensional vector. We note that one does not need to specify the language of the input, as the model was trained such that text across languages with similar meanings will have embeddings with high dot product scores.\n",
    "\n",
    "(*Extract from the TensorFlowHub [online documentation](https://tfhub.dev/google/universal-sentence-encoder-xling-many/1).*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sentence embeddings\n",
    "\n",
    "with np.load('data/text_embeddings_sentiment.npz', allow_pickle=False) as npz_file:\n",
    "    # Load the arrays\n",
    "    embeddings = npz_file['embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add embeddings to dataframe\n",
    "\n",
    "df_embed = pd.DataFrame(embeddings)\n",
    "df_embed['Text']  = df['tweet'].apply(lambda x: tools.clean_tweet_plot(x))\n",
    "df_embed.set_index('Text',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a few samples\n",
    "\n",
    "df_embed.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of embeddings\n",
    "\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question : The shape of `embeddings` is: (498,512). What does each dimension correspond to?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here:\n",
    "#\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise : Print the embedding for a tweet of your choice.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question : What is the length of the embeddings?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here\n",
    "# ...\n",
    "\n",
    "np.linalg.norm(embeddings,axis=1)[0:20]\n",
    "\n",
    "# Embeddings are normalized (length â‰ƒ 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean value of the high-level features\n",
    "\n",
    "fig,axes = plt.subplots(1,2,figsize=(12,4.5))\n",
    "\n",
    "axes[0].hist(np.mean(embeddings,axis=0),bins=50)\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_xlabel('Mean value');\n",
    "\n",
    "# Compute the std value of the high-level features\n",
    "\n",
    "axes[1].hist(np.std(embeddings,axis=0),bins=50)\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_xlabel('Standard deviation');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmin_dict = {'Twitter':0.09,\n",
    "             'politics':0.087,\n",
    "             'Nike':0.093,\n",
    "             'IT':0.092,\n",
    "             'sports':0.089,\n",
    "             'movies':0.098,\n",
    "             'cable TV':0.09,\n",
    "             'mobile devices':0.092,\n",
    "             'DSLR cameras':0.093,\n",
    "            'unlabeled':0.08,\n",
    "             'car industry':0.088,\n",
    "            'books':0.092,'cities':0.09,\n",
    "             'dentist':0.09,'blogging':0.09,\n",
    "            'food':0.09,'Warren Buffett':0.09, 'Bobby Flay':0.09}\n",
    "\n",
    "selected_categories = ['Twitter','movies','cities','IT','books']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(len(selected_categories),1,figsize=(20,3.5*len(selected_categories)))\n",
    "\n",
    "for cat,ax in zip(selected_categories,axes.ravel()):\n",
    "    \n",
    "    idx = df['semantic_category'] == cat\n",
    "    \n",
    "    X_plot = embeddings[idx]\n",
    "    vmin = vmin_dict[cat]\n",
    "    sns.heatmap(np.abs(X_plot),\n",
    "                vmin=vmin,\n",
    "                ax=ax)\n",
    "    ax.set_title(cat)\n",
    "    ax.set_yticks(ticks = np.arange(0,len(X_plot),3))\n",
    "    \n",
    "    ax.set_yticklabels(labels=[tools.clean_tweet_plot(tweet) for tweet in df.loc[idx,'tweet']][0:len(X_plot):3],rotation=0)\n",
    "    ax.set_xticks([])\n",
    "    \n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweets from similar, activate the same high-level feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export vectors\n",
    "\n",
    "np.savez('data/word_vectors.npz',BOW_features=X_bow.toarray(),tfidf_features=X_tfidf.toarray(),embeddings = embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
