{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google colab setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title << Setup Google Colab by running this cell {display-mode: \"form\"}\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    # Clone GitHub repository\n",
    "    !git clone https://github.com/pxydi/text.git\n",
    "        \n",
    "    # Copy files required to run the code\n",
    "    !cp -r \"text/data\" \"text/tools.py\" .\n",
    "    \n",
    "    # Install packages via pip\n",
    "    !pip install -r \"text/colab-requirements.txt\"\n",
    "    \n",
    "    # Restart Runtime\n",
    "    import os\n",
    "    os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text is highly unstructured and needs to be prepared into a form that can be processed by machine learning algorithms. There are several different [approaches](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction) for extracting features from text and we will explore a few of them in the next notebook. However, before extracting features from text, we need to \"preprocess\" it, i.e., \"clean\" and \"standardize\" it. This is because raw text can be \"messy\", especially when coming from social media platforms! We need to keep as many \"informative\" words as possible while discarding the \"uninformative\" ones. Removing unnecessary terms, i.e., the \"noise\", will improve our models' performance.\n",
    "\n",
    "We will be using the [Sentiment140](http://help.sentiment140.com/for-students/) public twitter corpus. This dataset contains ~500 tweets, labeled as positive, negative, or neutral. The dataset is available in the *data* folder."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os, re, random, string\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "from gensim.models import Phrases\n",
    "\n",
    "pd.set_option('display.max_colwidth',None)\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os, re, random, string\n",
    "from collections import defaultdict\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "from gensim.models import Phrases\n",
    "\n",
    "# Load helper functions\n",
    "import tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load corpus (twitter dataset)\n",
    "\n",
    "path = os.path.join('data','sentiment_140.csv')\n",
    "df   = pd.read_csv(path, header=None)\n",
    "\n",
    "# Rename columns\n",
    "df.columns = ['label','tweet']\n",
    "\n",
    "# Re-order columns\n",
    "df = df[['tweet','label']].copy()\n",
    "\n",
    "# Drop duplicates\n",
    "df = df.drop_duplicates('tweet')\n",
    "\n",
    "# Remove empty rows\n",
    "df = df.dropna()\n",
    "\n",
    "# Rename labels\n",
    "label_dict = {0:'neg', 2:'neutral', 4:'pos'}\n",
    "df['label'] = df['label'].replace(label_dict)\n",
    "\n",
    "# Reset index\n",
    "df.reset_index(inplace=True,drop=True)\n",
    "print('Data size: ',df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Take a few minutes to look at the raw text. What do you think we should remove from the text?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many different ways for text preprocessing, depending on the application and the text's language. Below are a few suggestions of what we could address in this particular dataset. However, keep in mind that you may have to adapt the techniques discussed in this notebook to your specific case.\n",
    "\n",
    "* remove URLs (e.g., http://bit.ly/19epAH, www.tinyurl.com/m595fk)\n",
    "* remove RT (stands for retweet)\n",
    "* remove Twitter usernames (e.g., @BlondeBroad)\n",
    "* remove hashtags (e.g. #Adidas -> Adidas)\n",
    "* remove punctuation. However, a few groupings, such as `:-), <3, : d`, etc., express emotion, so, depending on the task, we may want to keep them in the text.\n",
    "* remove numbers (e.g. 2020, 2, 15, ...)\n",
    "* perform case conversion (e.g. Good -> good, ...)\n",
    "* remove stopwords\n",
    "* remove non-ASCII characters\n",
    "* standardize the number of repeated characters, e.g. (\"I loooooooovvvvvveee\" -> \"I loovvee\")\n",
    "* expand contractions (e.g. \"don't\" -> \"do not\", \"won't\" -> \"will not\", ...)\n",
    "* apply stemming\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Tokenization is the process of splitting text into **words**.\n",
    "\n",
    "Example:\n",
    "\n",
    "$$\\text{I came to Bern by train.} -> \\text{[I, came, to, Bern, by, train.]}$$\n",
    "  \n",
    "  \n",
    "**The *TweetTokenizer***\n",
    "\n",
    "The *TweetTokenizer* is a nice tool from the NLTK library, specially designed for tokenizing tweets. Apart from spliting text into words, it offers a few additional key options:\n",
    "- reduces the number of repeated characters within a token e.g. \"everrrrr\" -> \"everrr\" (use: *reduce_len=True*)\n",
    "- removes Twitter usernames (use: *strip_handles=True*)\n",
    "- preserves punctuation and emoticons.\n",
    "\n",
    "Let's see this with a few examples ([source](https://www.nltk.org/_modules/nltk/tokenize/casual.html#TweetTokenizer))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# Create an instance  of the tokenizer\n",
    "tokenizer = TweetTokenizer(reduce_len=True, strip_handles=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example using emoticons and punctuation\n",
    "\n",
    "sample_1 = \"This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--\"\n",
    "\n",
    "print(sample_1,'\\n')\n",
    "print(tokenizer.tokenize(sample_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example using strip_handles and reduce_len parameters\n",
    "\n",
    "sample_2 = '@remy: This is waaaaayyyy too much for you!!!!!!'\n",
    "\n",
    "print(sample_2,'\\n')\n",
    "print(tokenizer.tokenize(sample_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case folding\n",
    "\n",
    "We usually convert all documents to lowercase. This is because we want our models to count e.g. \"I\" together with \"i\", \"The\" together with \"the\", etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_3 = \"I don't like this movie!\"\n",
    "\n",
    "print('Original tweet: \\t\\t\\t{}\\n'.format(sample_3))\n",
    "\n",
    "# Case folding\n",
    "print('Convert to lowercase: \\t\\t\\t{}\\n'.format(sample_3.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "\n",
    "Stopwords are words that are essential for a sentence to make sense, such as: \"I\", \"the\", \"and\", etc. The issue with stopwords is that they are: *very frequent* and *uninformative*. For most NLP applications, it is a good idea to remove them from text. \n",
    "\n",
    "Most NLP libraries provide pre-compiled lists of stopwords for several languages. In this notebook, we will use the list provided by the [NLTK library](https://www.nltk.org/).Â "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load english stopwords from nltk\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords          \n",
    "stopwords_english = stopwords.words('english') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print stopwords\n",
    "\n",
    "print('{} stopwords in NLTK\\'s list.\\n'.format(len(stopwords_english)))\n",
    "print(stopwords_english)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the NLTK stopword list includes negation words such as:\n",
    "\n",
    "- no, nor, not\n",
    "- don't, didn't, wouldn't\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if 'not' is in stopwords_english\n",
    "\n",
    "'not' in stopwords_english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if \"don't\" is in stopwords_english\n",
    "\n",
    "\"don't\" in stopwords_english"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may be an important point to keep in mind, depending on the application. For example, if we use the NLTK stopword list \"out-of-the-box\", then a sentence like: \"I don't like this movie\" will become: \"like this movie\". We see that the processed sentence conveys the exact opposite sentiment from the original one! \n",
    "\n",
    "**We need to be aware of the limitations of pre-compiled stopwords lists; making sure that we adapt them to our particular needs may be necessary for some specific tasks (e.g. sentiment analysis, product reviews, etc.).**\n",
    "\n",
    "We will \"customize\" the NLTK stopword list to ensure that we don't remove negation words from tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove negation words from nltk's stopword list\n",
    "\n",
    "not_stopwords = {'no', 'nor', 'not'} \n",
    "custom_stopwords = set([word for word in stopwords_english if word not in not_stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if 'not' is in custom_stopwords\n",
    "\n",
    "'not' in custom_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, customizing the NLTK stopword lists will not be enough for preserving negation words in the text. We should also expand contractions: e.g. \"don't\" -> \"do not\". We can do this with the library \"expand_contractions\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install contractions\n",
    "import contractions\n",
    "\n",
    "sample_3 = \"I don't like this movie!\"\n",
    "print('Original tweet: \\t\\t\\t{}\\n'.format(sample_3))\n",
    "\n",
    "# Case folding\n",
    "print('Convert to lowercase: \\t\\t\\t{}\\n'.format(sample_3.lower()))\n",
    "\n",
    "# Expand contractions\n",
    "sample_3_expanded = contractions.fix(sample_3.lower())\n",
    "print('Expand contractions: \\t\\t\\t{}\\n'.format(sample_3_expanded))\n",
    "\n",
    "# Tokenize tweet\n",
    "print('Tokenize: \\t\\t\\t\\t{}\\n'.format(sample_3_expanded.split()))\n",
    "\n",
    "# Remove stopwords\n",
    "print('Remove stopwords: \\t\\t\\t{}\\n'.format([w for w in sample_3_expanded.split() if w not in custom_stopwords]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We made sure that the \"important\" words for guessing the sentiment of this tweet (i.e. \"not\" and \"like\") were preserved!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Another part of text normalization is stemming, in which we mainly strip suffixes from the end of the word. The Porter stemmer is a widely used stemming tool for the English language. Stemming helps to connect words and reduce the size of the vocabulary (i.e., the number of unique words in a corpus). However, it can produce non-words, i.e., words that you won't find in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stemmer\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stem of \"retrieve\"\n",
    "\n",
    "stemmer.stem('retrieve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stem of \"retrieval\"\n",
    "\n",
    "stemmer.stem('retrieval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stem of \"retrieved\"\n",
    "\n",
    "stemmer.stem('retrieved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The process_tweet function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's bring everything together and create the `process_tweet` funchion which takes a tweet as an argument and  preprocesses it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create process_tweet function\n",
    "\n",
    "def process_tweet(tweet):\n",
    "    \n",
    "    '''\n",
    "    Preprocess raw samples of tweets.\n",
    "    \n",
    "    INPUT: \n",
    "    - tweet: raw text (string)\n",
    "    \n",
    "    OUTPUT:\n",
    "    - processed_tweet: processed tweet (string)\n",
    "    '''\n",
    "    \n",
    "    # Remove RT\n",
    "    clean_tweet = re.sub(r'RT','',tweet)\n",
    "\n",
    "    # Remove URL\n",
    "    clean_tweet = re.sub(r'https?:\\/\\/[^\\s]+','',clean_tweet)\n",
    "\n",
    "    # Remove hash #\n",
    "    clean_tweet = re.sub(r'#','',clean_tweet)\n",
    "\n",
    "    # Remove numbers\n",
    "    clean_tweet = re.sub(r'\\d+','',clean_tweet)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    clean_tweet = clean_tweet.lower()\n",
    "    \n",
    "    # Remove punctuation repetions (that are not removed by TweetTokenizer)\n",
    "    clean_tweet = re.sub(r'([._]){2,}','',clean_tweet)\n",
    "    \n",
    "    # Remove non-ascii chars\n",
    "    clean_tweet = ''.join([c for c in str(clean_tweet) if ord(c) < 128])\n",
    "\n",
    "    # Expand contractions\n",
    "    clean_tweet = contractions.fix(clean_tweet)\n",
    "    \n",
    "    # Tokenize tweet\n",
    "    tokens = tokenizer.tokenize(clean_tweet)\n",
    "\n",
    "    # Remove punctuation (except emoticons), stopwords, single-char words and apply stemming\n",
    "    clean_tokens = [stemmer.stem(w) for w in tokens if (w not in string.punctuation) and\n",
    "                       (w not in custom_stopwords) and (len(w)>1)]\n",
    "    \n",
    "    # The stemmer strips the final 's but leaves the apostroph: warner's -> warner'\n",
    "    # Here, I'm removing the apostroph from the end of words\n",
    "    clean_tokens = [tok if tok[-1] != \"'\" else tok[:-1] for tok in clean_tokens]\n",
    "\n",
    "    # Join tokens in a single string to recreate the tweet\n",
    "    processed_tweet = ' '.join([tok for tok in clean_tokens])\n",
    "    processed_tweet = processed_tweet.strip()\n",
    "       \n",
    "    return processed_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this function with a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a tweet randomly from the corpus\n",
    "\n",
    "tweet =  df.iloc[random.randint(0,len(df)-1),0]\n",
    "\n",
    "print('Before cleaning: \\t{}\\n'.format(tweet))\n",
    "print('After cleaning: \\t{}\\n'.format(process_tweet(tweet)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collocations\n",
    "\n",
    "There is one last thing we can do before moving on to feature extraction.\n",
    "\n",
    "We can look for combinations of words that frequently appear together, such as: \"Night at the museum\", \"Star Trek\", \"last night\", \"San Francisco\", \"North Korea\", etc, and replace them by a unique token. \n",
    "\n",
    "Example: \n",
    "\n",
    "$$\\text{\"Star Trek\" -> \"Star_Trek\"}$$\n",
    "$$\\text{\"San Francisco\" -> \"San_Francisco\"}$$\n",
    "$$\\text{\"last night\" -> \"last_night\"}$$\n",
    "\n",
    "\n",
    "We often call these *phrases* or *collocations*; they are word combinations that are more common in the corpus than the individual words themselves. (*Note: \"that is\" is not considered a collocation*).\n",
    "\n",
    "We will use Gensim's `models.phrases` to detect phrases (collocations) in our corpus. \"Phrases\" will identify the most common collocations and join the constituent tokens into a single token, using the \"_\" glue character. \n",
    "\n",
    "*Documentation*\n",
    "* Gensim's website: https://radimrehurek.com/gensim/models/phrases.html\n",
    "* Mikolov, *et. al*: [\"Distributed Representations of Words and Phrases and their Compositionality\"](https://arxiv.org/pdf/1310.4546.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all tweets (and add to list)\n",
    "\n",
    "processed_tweets = [process_tweet(tweet).split() for tweet in df['tweet']]\n",
    "\n",
    "# Detect collocations in corpus\n",
    "\n",
    "for i in ['bigrams']:\n",
    "    print('Computing collocations ({})...'.format(i))\n",
    "    \n",
    "    bigram = gensim.models.Phrases(processed_tweets,   # Expected format: list of tokenized documents\n",
    "                                   min_count=3,        # Ignore all words and bigrams with total collected count lower than this value.\n",
    "                                   delimiter=b'_')     # Glue character used to join collocation tokens\n",
    "\n",
    "    bigram_model = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "    # Add detected collocations to corpus\n",
    "    processed_tweets = [' '.join(bigram_model[processed_tweet]) for processed_tweet in processed_tweets]\n",
    "    print('Done!')\n",
    "    \n",
    "#Â Add processed tweets to dataframe\n",
    "df['processed_tweet'] = processed_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove  empty tweets\n",
    "df = df[df['processed_tweet'].apply(len) != 0].copy()\n",
    "\n",
    "# Reindex dataframe\n",
    "df.reset_index(inplace=True,drop=True)\n",
    "\n",
    "# Re-order columns\n",
    "df = df[['tweet','processed_tweet','label']]\n",
    "\n",
    "print('Data size: ',df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how the processed tweets look like: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's find out which collocations were detected in the corpus!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small exercise : Practice how to count word occurencies\n",
    "\n",
    "# Sort collocations by frequency of appearence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collocations_dict = defaultdict(int)\n",
    "\n",
    "for tweet in df['processed_tweet']:\n",
    "    for word in tweet.split():\n",
    "        if '_' in word:\n",
    "            collocations_dict[word] += 1\n",
    "            \n",
    "# Sort counts in descending order\n",
    "collocations_dict = {k:v for k,v in sorted(collocations_dict.items(), \n",
    "                                         key=lambda item:item[1], reverse=True)}\n",
    "            \n",
    "print('Found: {} collocations in corpus\\n'.format(len(list(collocations_dict.keys()))))\n",
    "\n",
    "# Print collocations (and frequency)\n",
    "collocations_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify (manually) tweets into \"semantic\" categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a later notebook, we will see how to identify semantically similar tweets, i.e., discussing related topics or themes. \n",
    "\n",
    "For the purpose of this analysis, we need to add tweets into semantic categories manually, based on selected keywords.\n",
    "\n",
    "For example, we will add any tweet mentioning movie-related terms (e.g. \"watch_night\", \"star_treck\", \"movi\", etc.) to the category: \"movies\". Tweets mentioning politics-related terms (e.g. \"north_korea\", \"obama\", \"pelosi\", \"bush\", \"china\", \"india\", \"iran\", etc.) will be added to \"politics\". Tweets mentionning sports-related terms (e.g. \"lebron\", \"laker\", \"basebal\", \"basketbal\", \"fifa\", \"ncaa\", \"roger\", \"feder\", etc.) will be added to \"sports\". And so on.\n",
    "\n",
    "We will use the function `add_to_semantic_category` to manually classify into \"semantic\" categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function add_to_semantic_category() \n",
    "\n",
    "df = tools.add_to_semantic_category(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a few samples\n",
    "\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data\n",
    "\n",
    "df.to_csv('data/clean_sentiment_140.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises for this afternoon\n",
    "\n",
    "1. Find the most common words in positive, negative and neutral tweets. Plot them in a meaningful way. Tip: Use the processed tweets!\n",
    "2. Select a few terms of interest that you would like to visualize. Count how many times each of these words appears in positive tweets and how many times in negative tweets. Then use a scatter plot to visualize your results (plot positive counts on x-axis and negative counts on y-axis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution # 1\n",
    "\n",
    "# Word frequencies\n",
    "\n",
    "# Positive tweets\n",
    "freq_pos = defaultdict(int)  # Defaultdict of type int: returns \"zero\" value of int if key doesn't exist\n",
    "\n",
    "for tweet in df.loc[df['label'] == 'pos','processed_tweet'].values:\n",
    "    for word in tweet.split():\n",
    "        freq_pos[word] += 1\n",
    "\n",
    "# Sort counts in descending order\n",
    "freq_pos = defaultdict(int,{k:v for k,v in sorted(freq_pos.items(),\n",
    "                                  key=lambda item:item[1], reverse=True)})\n",
    "\n",
    "# Negative tweets\n",
    "freq_neg = defaultdict(int)\n",
    "\n",
    "for tweet in df.loc[df['label'] == 'neg','processed_tweet'].values:\n",
    "    for word in tweet.split():\n",
    "        freq_neg[word] += 1\n",
    "        \n",
    "# Sort counts in descending order\n",
    "freq_neg = defaultdict(int,{k:v for k,v in sorted(freq_neg.items(),\n",
    "                                  key=lambda item:item[1], reverse=True)})\n",
    "\n",
    "# Plot most frequent words\n",
    "tools.plot_most_frequent_terms(frequency_dict = freq_pos, terms_to_plot = 50, add_to_title='in positive tweets')\n",
    "tools.plot_most_frequent_terms(frequency_dict = freq_neg, terms_to_plot = 50, add_to_title='in negative tweets')       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 2\n",
    "\n",
    "selected_words =['hate','love','good',':)',':(','dentist','kindl',\n",
    "                 'exam','not','star_trek','night_museum','north_korea',\n",
    "                 ':d','bush','time_warner']\n",
    "\n",
    "# Plot frequencies\n",
    "plt.figure(figsize=(6,6))\n",
    "for word in selected_words:\n",
    "    plt.scatter(np.log1p(freq_pos[word]),np.log1p(freq_neg[word]))\n",
    "    plt.annotate(word,(np.log1p(freq_pos[word])+0.15,np.log1p(freq_neg[word])),fontsize=13)\n",
    "\n",
    "plt.plot([-2, 5], [-2, 5], color = 'blue') \n",
    "plt.xlabel('Positive counts',fontsize=13,weight='bold')\n",
    "plt.ylabel('Negative counts',fontsize=13,weight='bold')\n",
    "plt.xlim([-0.5,4])\n",
    "plt.ylim([-0.5,4]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
